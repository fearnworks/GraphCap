# SPDX-License-Identifier: Apache-2.0
"""Type definitions for Hugging Face assets.

This module defines the Pydantic models and Dagster config classes
used for interacting with the Hugging Face Hub.

Classes:
    DatasetMetadata: Metadata for a dataset.
    DatasetExportManifestMetadata: Metadata for a dataset export manifest.
    HfUploadManifestMetadata: Metadata for a Hugging Face upload manifest.
    HfUploadManifestConfig: Configuration for uploading a manifest to the Hugging Face Hub.
    DatasetImportConfig: Configuration for importing datasets from the Hugging Face Hub.
    DatasetParseConfig: Configuration for parsing downloaded datasets.
    DatasetParquetUrlDownloadConfig: Configuration for downloading URLs from a parquet dataset.
"""

from dataclasses import dataclass
from typing import Any, List, Optional

import dagster as dg
from pydantic import BaseModel, Field


class DatasetMetadata(BaseModel):
    """
    Metadata for a dataset.

    Attributes:
        dataset_name (str): Name of the dataset.
        dataset_description (str): Description of the dataset.
        dataset_tags (List[str]): Tags for the dataset.
    """

    dataset_name: str = Field("graphcap_dataset", description="Name of the dataset")
    dataset_description: str = Field("A dataset generated by graphcap.", description="Description of the dataset")
    dataset_tags: List[str] = Field(
        default_factory=lambda: ["image-captioning", "computer-vision"], description="Tags for the dataset"
    )


class DatasetExportManifestMetadata(BaseModel):
    """
    Metadata for a dataset export manifest.

    Attributes:
        export_dir (str): The directory where the dataset export manifest is located.
        manifest_path (str): The path to the dataset export manifest file.
    """

    export_dir: str
    manifest_path: str


class HfUploadManifestMetadata(BaseModel):
    """
    Metadata for a Hugging Face upload manifest.

    Attributes:
        dataset_name (str): Name of the dataset.
        namespace (str): Hugging Face Hub namespace (user or organization).
        dataset_url (str): URL of the uploaded dataset on the Hugging Face Hub.
    """

    dataset_name: str
    namespace: str
    dataset_url: str


class HfUploadManifestConfig(dg.ConfigurableResource):
    """
    Configuration for uploading a manifest to the Hugging Face Hub.

    Attributes:
        dataset_name (str): Name of the dataset.
        namespace (str): Hugging Face Hub namespace (user or organization).
        dataset_description (str): Description of the dataset.
        dataset_tags (List[str]): Tags for the dataset.
        include_images (bool): Whether to include images in the dataset.
        use_hf_urls (bool): Whether to use Hugging Face URLs.
        private_dataset (bool): Whether the dataset should be private.
    """

    dataset_name: str = Field("initial-test-dataset", description="Name of the dataset")
    namespace: str = Field("openmodelinitiative", description="Hugging Face Hub namespace (user or organization)")
    dataset_description: str = Field("A dataset generated by graphcap.", description="Description of the dataset")
    dataset_tags: List[str] = Field(
        default_factory=lambda: ["image-captioning", "computer-vision"], description="Tags for the dataset"
    )
    include_images: bool = Field(True, description="Whether to include images in the dataset")
    use_hf_urls: bool = Field(False, description="Whether to use Hugging Face URLs")
    private_dataset: bool = Field(False, description="Whether the dataset should be private")


class DatasetImportConfig(dg.Config):
    """
    Configuration for importing datasets from Hugging Face Hub.

    Attributes:
        repo_id: The ID of the Hugging Face Hub repository (e.g., 'openmodelinitiative/initial-test-dataset')
        local_dir: The local directory to save the downloaded dataset
        use_git_lfs: Whether to clone the repository using Git LFS (recommended for full datasets)
        use_datasets_library: Whether to use the datasets library (recommended for standardized datasets)
        filename: Specific file to download (only used if not using git_lfs or datasets_library)
        clean_up: Whether to do a fresh clone of the repository
    """

    repo_id: str = Field(
        description="The ID of the Hugging Face Hub repository (e.g., 'openmodelinitiative/initial-test-dataset')."
    )
    local_dir: str = Field(
        default="/workspace/.local/output/dataset",
        description="The local directory to save the downloaded dataset.",
    )
    use_git_lfs: bool = Field(
        default=True,
        description="Whether to clone the repository using Git LFS (recommended for full datasets).",
    )
    use_datasets_library: bool = Field(
        default=False,
        description="Whether to use the datasets library (recommended for standardized datasets).",
    )
    filename: str | None = Field(
        default=None,
        description="Specific file to download (only used if not using git_lfs or datasets_library).",
    )
    fresh_clone: bool = Field(
        default=True,
        description="Whether to do a fresh clone of the repository.",
    )


class DatasetParseConfig(dg.Config):
    """
    Configuration for parsing downloaded datasets.

    Attributes:
        output_dir (str): The directory to save the parsed dataset.
            Defaults to "/workspace/.local/output/dataset".
    """

    output_dir: str = Field(
        default="/workspace/.local/output/dataset",
        description="The directory to save the parsed dataset.",
    )


@dataclass
class DatasetAnnotation:
    """Annotation in the dataset."""

    id: str
    content: str
    annotation: dict
    manuallyAdjusted: bool
    embedding: Optional[Any]
    fromUser: str
    fromTeam: str
    createdAt: str
    updatedAt: str
    overallRating: Optional[Any]


@dataclass
class DatasetRow:
    """Schema for a row in the dataset."""

    id: str
    type: str
    hash: str
    phash: str
    urls: List[str]
    status: str
    flags: int
    meta: dict
    fromUser: str
    fromTeam: str
    embeddings: List[Any]
    createdAt: str
    updatedAt: str
    name: Optional[str]
    width: Optional[int]
    height: Optional[int]
    format: Optional[str]
    license: Optional[str]
    licenseUrl: Optional[str]
    contentAuthor: Optional[str]
    annotations: List[DatasetAnnotation]
    image_column: Optional[str]


class DatasetParquetUrlDownloadConfig(dg.Config):
    """
    Configuration for downloading URLs from parquet datasets.

    Attributes:
        parquet_dir: Directory containing parquet files relative to dataset root
        url_column: Name of the column containing URLs (defaults to 'urls')
        output_dir: Directory to save downloaded files
        max_workers: Maximum number of concurrent downloads
        overwrite_existing: Whether to overwrite existing files
        default_extension: Default file extension when none can be determined from URL
        request_delay: Delay between requests in seconds
    """

    parquet_dir: str = Field(
        default="data",
        description="Directory containing parquet files relative to dataset root",
    )
    url_column: str = Field(
        default="urls",
        description="Name of the column containing URLs",
    )
    output_dir: str = Field(
        default="/workspace/.local/output/dataset/downloads",
        description="Directory to save downloaded files",
    )
    max_workers: int = Field(
        default=2,  # Reduced default for rate limiting
        description="Maximum number of concurrent downloads",
    )
    overwrite_existing: bool = Field(
        default=False,
        description="Whether to overwrite existing files",
    )
    default_extension: str = Field(
        default="jpg",
        description="Default file extension when none can be determined from URL",
    )
    request_delay: float = Field(
        default=1.0,
        description="Delay between requests in seconds",
    )
